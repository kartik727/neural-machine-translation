{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq-attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOYJT2ySCakjY3r/3rdYOWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartik727/neural-machine-translation/blob/master/Seq2Seq_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9w8BVLFXfjP"
      },
      "source": [
        "# Seq2Seq with attention\n",
        "\n",
        "## Dependencies\n",
        "Primary library used for modelling and training - trax\n",
        "\n",
        "## Data - Tensorflow Datasets (TFDS)\n",
        "1. OPUS (`'opus'`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfJ3KATuZxeD"
      },
      "source": [
        "# instal trax\n",
        "\n",
        "# !pip install trax"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIxeipWjVbKF"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as fastnp\n",
        "from trax.supervised import training"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1RFYK1rZngR"
      },
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHTVskzWBErB"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyaDxYcV1Z-3"
      },
      "source": [
        "from collections import defaultdict"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWGQout8gG-H"
      },
      "source": [
        "dataset_train = tfds.load('opus', split='train', batch_size=-1, shuffle_files=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlZsGEQ6v2dS"
      },
      "source": [
        "ds_np = tfds.as_numpy(dataset_train)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYIN8QK51Fr4"
      },
      "source": [
        "# Utils Namespace\n",
        "\n",
        "class Namespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.update(**kwargs)\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe2lkV32ruUS"
      },
      "source": [
        "config_dict = {\n",
        "    'vocab_size_en' : 20_000,\n",
        "    'vocab_size_de' : 20_000\n",
        "}\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-dx2pyK-_LL"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = w.decode().lower().strip()\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD9GeNrb_wRB"
      },
      "source": [
        "def preprocess_data(data):\n",
        "    return [preprocess_sentence(w) for w in data]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6_yEqno-_Hh"
      },
      "source": [
        "preprocessed_data_en, preprocessed_data_de = preprocess_data(ds_np['en'])[:10000],preprocess_data(ds_np['de'])[:10000]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDfv1M-VCOfT"
      },
      "source": [
        "del ds_np"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqtzCi9f-_Dj"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8oxRadQ-_BV"
      },
      "source": [
        "token_en = tokenize(preprocessed_data_en)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbXQE59tCW7X"
      },
      "source": [
        "del preprocessed_data_en"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI2QmRXm--_k"
      },
      "source": [
        "token_de = tokenize(preprocessed_data_de)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpMJBIwg--9r"
      },
      "source": [
        "def input_encoder_fn(input_vocab_size, embedding_size, n_encoder_layers):\n",
        "    input_encoder = tl.Serial( \n",
        "        tl.Embedding(input_vocab_size, embedding_size),\n",
        "        [tl.LSTM(embedding_size) for _ in range(n_encoder_layers)]\n",
        "    )\n",
        "    return input_encoder\n",
        "\n",
        "def pre_attention_decoder_fn(mode, target_vocab_size, embedding_size):\n",
        "    pre_attention_decoder = tl.Serial(\n",
        "        tl.ShiftRight(),\n",
        "        tl.Embedding(target_vocab_size, embedding_size),\n",
        "        tl.LSTM(embedding_size)\n",
        "    )\n",
        "    return pre_attention_decoder\n",
        "\n",
        "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
        "    keys = encoder_activations\n",
        "    values = encoder_activations\n",
        "    queries = decoder_activations\n",
        "    mask = inputs != 0\n",
        "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
        "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n",
        "    return queries, keys, values, mask\n",
        "\n",
        "def NMTAttn(input_vocab_size=33300,\n",
        "            target_vocab_size=33300,\n",
        "            embedding_size=1024,\n",
        "            n_encoder_layers=2,\n",
        "            n_decoder_layers=2,\n",
        "            n_attention_heads=4,\n",
        "            attention_dropout=0.0,\n",
        "            mode='train'):\n",
        "    input_encoder = input_encoder_fn(input_vocab_size, embedding_size, n_encoder_layers)\n",
        "    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, embedding_size)\n",
        "    model = tl.Serial( \n",
        "      tl.Select([0, 1, 0, 1]),\n",
        "      tl.Parallel(input_encoder, pre_attention_decoder),\n",
        "      tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\n",
        "      tl.Residual(tl.AttentionQKV(embedding_size, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\n",
        "      tl.Select([0, 2]),\n",
        "      [tl.LSTM(embedding_size) for _ in range(n_decoder_layers)],\n",
        "      tl.Dense(target_vocab_size),\n",
        "      tl.LogSoftmax()\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKfoLx41ZBIY"
      },
      "source": [
        "class Data_Iter:\n",
        "    def __init__(self, data_en, data_de, batch_size):\n",
        "        self.data_en = data_en\n",
        "        self.data_de = data_de\n",
        "        self.batch_size = batch_size\n",
        "        self.idx = 0\n",
        "        self.l = len(data_en)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.idx>=self.l:\n",
        "            self.idx=0\n",
        "        batch_en = self.data_en[self.idx:self.idx+self.batch_size]\n",
        "        batch_de = self.data_de[self.idx:self.idx+self.batch_size]\n",
        "        mask = np.array(batch_de != 0, dtype=int)\n",
        "        self.idx += self.batch_size\n",
        "        return batch_en, batch_de, mask"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcVPA6KpaMAe"
      },
      "source": [
        "data_iter = Data_Iter(token_en[0], token_de[0], 64)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3irSRq5Kj2p",
        "outputId": "38dff76b-3b50-46cc-8170-f862f0795cf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_task = training.TrainTask(\n",
        "    labeled_data= data_iter,\n",
        "    loss_layer= tl.CrossEntropyLoss(),\n",
        "    optimizer= trax.optimizers.Adam(learning_rate=0.01),\n",
        "    lr_schedule= trax.lr.warmup_and_rsqrt_decay(1000, 0.01),\n",
        "    n_steps_per_checkpoint= 10,\n",
        ")\n",
        "\n",
        "eval_task = training.EvalTask(\n",
        "    labeled_data=data_iter,\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL6yr6UAWTdp",
        "outputId": "ef81e3b9-1f02-4a1e-d6e3-a9ede8b8f9d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "output_dir = '/content/output_dir/'\n",
        "training_loop = training.Loop(NMTAttn(mode='train'),\n",
        "                              train_task,\n",
        "                              eval_tasks=[eval_task],\n",
        "                              output_dir=output_dir)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/lib/xla_bridge.py:304: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_id has been renamed to jax.process_index. This alias \"\n",
            "/usr/local/lib/python3.7/dist-packages/jax/lib/xla_bridge.py:317: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lQ_wHplavL4",
        "outputId": "c8778c4f-8bd4-4386-d785-06a87b2e14ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_loop.run(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/lib/xla_bridge.py:304: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_id has been renamed to jax.process_index. This alias \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ROdWzKEa5vY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}